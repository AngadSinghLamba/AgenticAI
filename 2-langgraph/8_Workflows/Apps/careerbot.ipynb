{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Import Required Libraries\n",
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# âœ… Load API Keys (Set up in .env file)\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# âœ… Initialize LLM (GPT-4 for Interview Questions & Analysis)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# âœ… Define AI State (Tracking Interview Progress)\n",
    "class InterviewState(TypedDict):\n",
    "    job_description: str\n",
    "    interview_questions: list\n",
    "    current_question: str\n",
    "    answer: str\n",
    "    feedback: str\n",
    "    score: int\n",
    "    final_feedback: str\n",
    "\n",
    "# âœ… Step 1: Generate Questions Based on Job Description\n",
    "generate_questions_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\"],\n",
    "    template=\"Based on this job description, generate 5 interview questions:\\n{job_description}\"\n",
    ")\n",
    "generate_questions_chain = LLMChain(llm=llm, prompt=generate_questions_prompt)\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    \"\"\"Creates interview questions based on job description\"\"\"\n",
    "    questions = generate_questions_chain.run(state[\"job_description\"]).split(\"\\n\")\n",
    "    return {\"interview_questions\": questions, \"current_question\": questions[0]}\n",
    "\n",
    "# âœ… Step 2: Analyze Candidate Answer\n",
    "analyze_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"current_question\", \"answer\"],\n",
    "    template=\"Evaluate this answer based on clarity, correctness, and depth.\\nQuestion: {current_question}\\nAnswer: {answer}\\nProvide a score out of 10.\"\n",
    ")\n",
    "analyze_answer_chain = LLMChain(llm=llm, prompt=analyze_answer_prompt)\n",
    "\n",
    "def analyze_answer(state: InterviewState):\n",
    "    \"\"\"Evaluates candidate's answer\"\"\"\n",
    "    score = int(analyze_answer_chain.run(state[\"current_question\"], state[\"answer\"]).split()[0])\n",
    "    return {\"score\": score}\n",
    "\n",
    "# âœ… Step 3: Provide Feedback on Answer\n",
    "feedback_prompt = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"score\"],\n",
    "    template=\"Provide constructive feedback on this answer based on its score ({score}/10).\\nAnswer: {answer}\"\n",
    ")\n",
    "feedback_chain = LLMChain(llm=llm, prompt=feedback_prompt)\n",
    "\n",
    "def provide_feedback(state: InterviewState):\n",
    "    \"\"\"Gives feedback based on the answer score\"\"\"\n",
    "    feedback = feedback_chain.run(state[\"answer\"], state[\"score\"])\n",
    "    return {\"feedback\": feedback}\n",
    "\n",
    "# âœ… Conditional Step: Retry Weak Answers or Proceed to Next Question\n",
    "def should_retry(state: InterviewState):\n",
    "    \"\"\"Determines if the candidate should retry the answer\"\"\"\n",
    "    if state[\"score\"] < 6:\n",
    "        return \"retry_question\"\n",
    "    elif len(state[\"interview_questions\"]) > 1:\n",
    "        # Move to the next question if available\n",
    "        state[\"interview_questions\"].pop(0)\n",
    "        state[\"current_question\"] = state[\"interview_questions\"][0]\n",
    "        return \"next_question\"\n",
    "    else:\n",
    "        return \"generate_final_feedback\"\n",
    "\n",
    "# âœ… Step 4: Generate Final Feedback\n",
    "final_feedback_prompt = PromptTemplate(\n",
    "    input_variables=[\"score\"],\n",
    "    template=\"Based on the interview performance with an average score of {score}/10, provide a final evaluation.\"\n",
    ")\n",
    "final_feedback_chain = LLMChain(llm=llm, prompt=final_feedback_prompt)\n",
    "\n",
    "def generate_final_feedback(state: InterviewState):\n",
    "    \"\"\"Generates final performance feedback\"\"\"\n",
    "    final_feedback = final_feedback_chain.run(state[\"score\"])\n",
    "    return {\"final_feedback\": final_feedback}\n",
    "\n",
    "# âœ… Build LangGraph Workflow\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# âž¤ Add Nodes\n",
    "workflow.add_node(\"generate_questions\", generate_questions)\n",
    "workflow.add_node(\"analyze_answer\", analyze_answer)\n",
    "workflow.add_node(\"provide_feedback\", provide_feedback)\n",
    "workflow.add_node(\"generate_final_feedback\", generate_final_feedback)\n",
    "\n",
    "# âž¤ Define Execution Flow with Conditional Routing\n",
    "workflow.add_edge(START, \"generate_questions\")  # Start â†’ Generate Questions\n",
    "workflow.add_edge(\"generate_questions\", \"analyze_answer\")  # Questions â†’ Analyze Answer\n",
    "\n",
    "# Conditional: If score < 6 â†’ Retry, else â†’ Next Question or Final Feedback\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_answer\",\n",
    "    should_retry,\n",
    "    {\"retry_question\": \"provide_feedback\", \"next_question\": \"analyze_answer\", \"generate_final_feedback\": \"generate_final_feedback\"}\n",
    ")\n",
    "\n",
    "# Retry process before moving to the next question\n",
    "workflow.add_edge(\"provide_feedback\", \"analyze_answer\")\n",
    "\n",
    "# End after generating final performance feedback\n",
    "workflow.add_edge(\"generate_final_feedback\", END)\n",
    "\n",
    "# âœ… Compile the Graph\n",
    "interview_graph = workflow.compile()\n",
    "\n",
    "# âœ… Streamlit Web App\n",
    "st.title(\"ðŸŽ¤ AI Mock Interview System\")\n",
    "st.markdown(\"**Enter a job description, and AI will generate questions & evaluate your answers!**\")\n",
    "\n",
    "# User input for job description\n",
    "job_description = st.text_area(\"Enter Job Description:\", \"Looking for a Python Developer with experience in Flask, SQL, and REST APIs.\")\n",
    "\n",
    "if st.button(\"Start Interview\"):\n",
    "    # Initialize state\n",
    "    state = {\"job_description\": job_description}\n",
    "    state = interview_graph.invoke(state)\n",
    "\n",
    "    st.session_state[\"state\"] = state\n",
    "\n",
    "if \"state\" in st.session_state:\n",
    "    st.markdown(f\"**Question:** {st.session_state['state'].get('current_question', 'N/A')}\")\n",
    "\n",
    "    # Candidate answers\n",
    "    answer = st.text_area(\"Your Answer:\")\n",
    "    if st.button(\"Submit Answer\"):\n",
    "        st.session_state[\"state\"][\"answer\"] = answer\n",
    "        st.session_state[\"state\"] = interview_graph.invoke(st.session_state[\"state\"])\n",
    "\n",
    "        # Show feedback\n",
    "        st.markdown(f\"**Feedback:** {st.session_state['state'].get('feedback', 'N/A')}\")\n",
    "        st.markdown(f\"**Score:** {st.session_state['state'].get('score', 0)}\")\n",
    "\n",
    "    # Show final performance evaluation\n",
    "    if \"final_feedback\" in st.session_state[\"state\"]:\n",
    "        st.markdown(f\"**Final Evaluation:** {st.session_state['state']['final_feedback']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(TypedDict):\n",
    "    job_description: str\n",
    "    interview_questions: list\n",
    "    current_question: str\n",
    "    answer: str\n",
    "    feedback: str\n",
    "    score: int\n",
    "    final_feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_questions_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\"],\n",
    "    template=\"Based on this job description, generate 5 interview questions:\\n{job_description}\"\n",
    ")\n",
    "generate_questions_chain = generate_questions_prompt|llm\n",
    "\n",
    "def generate_questions(state: InterviewState):\n",
    "    \"\"\"Creates interview questions based on job description\"\"\"\n",
    "    questions = generate_questions_chain.run(state[\"job_description\"]).split(\"\\n\")\n",
    "    return {\"interview_questions\": questions, \"current_question\": questions[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"current_question\", \"answer\"],\n",
    "    template=\"Evaluate this answer based on clarity, correctness, and depth.\\nQuestion: {current_question}\\nAnswer: {answer}\\nProvide a score out of 10.\"\n",
    ")\n",
    "analyze_answer_chain = analyze_answer_prompt|llm\n",
    "\n",
    "def analyze_answer(state: InterviewState):\n",
    "    \"\"\"Evaluates candidate's answer\"\"\"\n",
    "    score = int(analyze_answer_chain.run(state[\"current_question\"], state[\"answer\"]).split()[0])\n",
    "    return {\"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_prompt = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"score\"],\n",
    "    template=\"Provide constructive feedback on this answer based on its score ({score}/10).\\nAnswer: {answer}\"\n",
    ")\n",
    "feedback_chain = feedback_prompt|llm\n",
    "\n",
    "def provide_feedback(state: InterviewState):\n",
    "    \"\"\"Gives feedback based on the answer score\"\"\"\n",
    "    feedback = feedback_chain.run(state[\"answer\"], state[\"score\"])\n",
    "    return {\"feedback\": feedback}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_retry(state: InterviewState):\n",
    "    \"\"\"Determines if the candidate should retry the answer\"\"\"\n",
    "    if state[\"score\"] < 6:\n",
    "        return \"retry_question\"\n",
    "    elif len(state[\"interview_questions\"]) > 1:\n",
    "        # Move to the next question if available\n",
    "        state[\"interview_questions\"].pop(0)\n",
    "        state[\"current_question\"] = state[\"interview_questions\"][0]\n",
    "        return \"next_question\"\n",
    "    else:\n",
    "        return \"generate_final_feedback\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feedback_prompt = PromptTemplate(\n",
    "    input_variables=[\"score\"],\n",
    "    template=\"Based on the interview performance with an average score of {score}/10, provide a final evaluation.\"\n",
    ")\n",
    "final_feedback_chain = LLMChain(llm=llm, prompt=final_feedback_prompt)\n",
    "\n",
    "def generate_final_feedback(state: InterviewState):\n",
    "    \"\"\"Generates final performance feedback\"\"\"\n",
    "    final_feedback = final_feedback_chain.run(state[\"score\"])\n",
    "    return {\"final_feedback\": final_feedback}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# âž¤ Add Nodes\n",
    "workflow.add_node(\"generate_questions\", generate_questions)\n",
    "workflow.add_node(\"analyze_answer\", analyze_answer)\n",
    "workflow.add_node(\"provide_feedback\", provide_feedback)\n",
    "workflow.add_node(\"generate_final_feedback\", generate_final_feedback)\n",
    "\n",
    "# âž¤ Define Execution Flow with Conditional Routing\n",
    "workflow.add_edge(START, \"generate_questions\")  \n",
    "workflow.add_edge(\"generate_questions\", \"analyze_answer\")  \n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_answer\",\n",
    "    should_retry,\n",
    "    {\"retry_question\": \"provide_feedback\", \"next_question\": \"analyze_answer\", \"generate_final_feedback\": \"generate_final_feedback\"}\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"provide_feedback\", \"analyze_answer\")\n",
    "workflow.add_edge(\"generate_final_feedback\", END)\n",
    "\n",
    "# âœ… Compile the Graph\n",
    "interview_graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minterview_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mAI Engineer will contributing to the development and deployment of NLP, LLM and Machine Learning. AI Engineer will play a crucial role in advancing our Generative AI capabilities and driving innovation across multiple projects. The person will collaborate closely with cross-functional teams to design, implement, and optimize algorithms that generate high-quality content. Experience working with various genAI toolkits like Langchain, LlamaIndex, Huggingface Transformers, Spacy, CoreNLP, OpenNLP etc. Proven experience with natural language processing, including textual data processing , training, and evaluating deep learning and large language models (Transformer based models, Bert, GPT, etc Languages: Python, AQL Platform: Azure, AWS Libraries: NLTK, scikit-learn. Hugging Face transformers, spaCy LLM - GPT3, GPT4 Primary Skills: GenAI, LLM, Cloud (AWS), Data Science, NLP\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2142\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2141\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2142\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2147\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1797\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1804\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_questions\u001b[39m(state: InterviewState):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates interview questions based on job description\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_description\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterview_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m: questions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_question\u001b[39m\u001b[38;5;124m\"\u001b[39m: questions[\u001b[38;5;241m0\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\agenticai_env\\Lib\\site-packages\\pydantic\\main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'run'",
      "\u001b[0mDuring task with name 'generate_questions' and id 'f671b63e-3141-77f2-f938-81c7d1dc01bd'"
     ]
    }
   ],
   "source": [
    "interview_graph.invoke({\"job_description\":\"\"\"AI Engineer will contributing to the development and deployment of NLP, LLM and Machine Learning. AI Engineer will play a crucial role in advancing our Generative AI capabilities and driving innovation across multiple projects. The person will collaborate closely with cross-functional teams to design, implement, and optimize algorithms that generate high-quality content. Experience working with various genAI toolkits like Langchain, LlamaIndex, Huggingface Transformers, Spacy, CoreNLP, OpenNLP etc. Proven experience with natural language processing, including textual data processing , training, and evaluating deep learning and large language models (Transformer based models, Bert, GPT, etc Languages: Python, AQL Platform: Azure, AWS Libraries: NLTK, scikit-learn. Hugging Face transformers, spaCy LLM - GPT3, GPT4 Primary Skills: GenAI, LLM, Cloud (AWS), Data Science, NLP\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
